{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VLM Cross-Modal Deep Hashing — Colab Launcher\n",
    "\n",
    "Train a cross-modal hashing model (SigLIP2 → 1-bit binary codes) with real-time monitoring dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: Tesla T4 (14.7 GB)\n",
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: GPU Check + Google Drive Mount\n",
    "import torch\n",
    "\n",
    "assert torch.cuda.is_available(), \"No GPU detected — switch to a GPU runtime.\"\n",
    "gpu_name = torch.cuda.get_device_name(0)\n",
    "vram = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "print(f\"GPU: {gpu_name} ({vram:.1f} GB)\")\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "!mkdir -p /content/drive/MyDrive/vlm_quantization/checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path '/content/vlm_quantization' already exists and is not an empty directory.\n",
      "/content/vlm_quantization\n",
      ".env loaded from Google Drive\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Clone Repo + Install Dependencies + Load .env from Google Drive\n",
    "!git clone https://github.com/hyunlord/vlm_quantization.git /content/vlm_quantization\n",
    "%cd /content/vlm_quantization\n",
    "!pip install -q -r requirements.txt\n",
    "!pip install -q pyngrok\n",
    "\n",
    "import os\n",
    "\n",
    "env_path = \"/content/drive/MyDrive/vlm_quantization/.env\"\n",
    "if os.path.exists(env_path):\n",
    "    with open(env_path) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line and not line.startswith(\"#\") and \"=\" in line:\n",
    "                key, val = line.split(\"=\", 1)\n",
    "                os.environ[key.strip()] = val.strip()\n",
    "    print(f\".env loaded from Google Drive\")\n",
    "else:\n",
    "    print(f\"No .env found at {env_path} — create one with NGROK_AUTH_TOKEN if needed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Cell 3: Load COCO Captions + Karpathy Split (zip cached on Drive, extracted to local SSD)\n",
    "# Strategy: Drive FUSE can't handle 82K small files reliably.\n",
    "# → Cache zips on Drive (single large files = safe)\n",
    "# → Extract to local /content/data/coco each session (fast SSD)\n",
    "import os, shutil\n",
    "\n",
    "DRIVE_CACHE = \"/content/drive/MyDrive/data/coco_zips\"  # zip cache on Drive\n",
    "LOCAL_COCO  = \"/content/data/coco\"                      # training reads from here\n",
    "\n",
    "SOURCES = {\n",
    "    \"train2014\": {\n",
    "        \"url\": \"http://images.cocodataset.org/zips/train2014.zip\",\n",
    "        \"zip\": \"train2014.zip\",\n",
    "        \"folder\": \"train2014\",\n",
    "    },\n",
    "    \"val2014\": {\n",
    "        \"url\": \"http://images.cocodataset.org/zips/val2014.zip\",\n",
    "        \"zip\": \"val2014.zip\",\n",
    "        \"folder\": \"val2014\",\n",
    "    },\n",
    "    \"annotations\": {\n",
    "        \"url\": \"http://images.cocodataset.org/annotations/annotations_trainval2014.zip\",\n",
    "        \"zip\": \"annotations_trainval2014.zip\",\n",
    "        \"folder\": \"annotations\",\n",
    "    },\n",
    "}\n",
    "\n",
    "os.makedirs(DRIVE_CACHE, exist_ok=True)\n",
    "os.makedirs(LOCAL_COCO, exist_ok=True)\n",
    "\n",
    "for i, (name, src) in enumerate(SOURCES.items(), 1):\n",
    "    local_dir = f\"{LOCAL_COCO}/{src['folder']}\"\n",
    "    drive_zip = f\"{DRIVE_CACHE}/{src['zip']}\"\n",
    "    tmp_zip   = f\"/tmp/{src['zip']}\"\n",
    "\n",
    "    if os.path.isdir(local_dir):\n",
    "        print(f\"  [{i}/3] {name} — already extracted locally, skipping\")\n",
    "        continue\n",
    "\n",
    "    # Try Drive cache first, else download from web\n",
    "    if os.path.isfile(drive_zip):\n",
    "        print(f\"  [{i}/3] {name} — copying cached zip from Drive...\")\n",
    "        shutil.copy2(drive_zip, tmp_zip)\n",
    "    else:\n",
    "        print(f\"  [{i}/3] {name} — downloading...\")\n",
    "        !wget -q --show-progress {src['url']} -O {tmp_zip}\n",
    "        # Cache zip to Drive for next session\n",
    "        print(f\"         caching zip to Drive...\")\n",
    "        shutil.copy2(tmp_zip, drive_zip)\n",
    "\n",
    "    # Extract to local SSD (fast, no FUSE issues)\n",
    "    print(f\"         extracting to local disk...\")\n",
    "    !unzip -q {tmp_zip} -d {LOCAL_COCO}/\n",
    "    os.remove(tmp_zip)\n",
    "\n",
    "# Karpathy split JSON (dataset_coco.json)\n",
    "KARPATHY_JSON = f\"{LOCAL_COCO}/dataset_coco.json\"\n",
    "if not os.path.isfile(KARPATHY_JSON):\n",
    "    KARPATHY_URL = \"https://cs.stanford.edu/people/karpathy/deepimagesent/caption_datasets.zip\"\n",
    "    karpathy_zip = \"/tmp/caption_datasets.zip\"\n",
    "    drive_karpathy = f\"{DRIVE_CACHE}/caption_datasets.zip\"\n",
    "\n",
    "    if os.path.isfile(drive_karpathy):\n",
    "        print(\"  [K] Karpathy JSON — copying cached zip from Drive...\")\n",
    "        shutil.copy2(drive_karpathy, karpathy_zip)\n",
    "    else:\n",
    "        print(\"  [K] Karpathy JSON — downloading...\")\n",
    "        !wget -q --show-progress {KARPATHY_URL} -O {karpathy_zip}\n",
    "        print(\"         caching zip to Drive...\")\n",
    "        shutil.copy2(karpathy_zip, drive_karpathy)\n",
    "\n",
    "    print(\"         extracting dataset_coco.json...\")\n",
    "    !unzip -q -j {karpathy_zip} \"dataset_coco.json\" -d {LOCAL_COCO}/\n",
    "    os.remove(karpathy_zip)\n",
    "else:\n",
    "    print(\"  [K] Karpathy JSON — already present, skipping\")\n",
    "\n",
    "# Verify\n",
    "for name in (\"train2014\", \"val2014\", \"annotations\"):\n",
    "    assert os.path.isdir(f\"{LOCAL_COCO}/{name}\"), f\"{name} missing!\"\n",
    "assert os.path.isfile(KARPATHY_JSON), \"dataset_coco.json missing!\"\n",
    "\n",
    "import json\n",
    "with open(KARPATHY_JSON) as f:\n",
    "    kdata = json.load(f)\n",
    "splits = {}\n",
    "for img in kdata[\"images\"]:\n",
    "    s = img[\"split\"]\n",
    "    splits[s] = splits.get(s, 0) + 1\n",
    "del kdata\n",
    "\n",
    "print(f\"\\nCOCO ready: {LOCAL_COCO}\")\n",
    "print(f\"  train2014: {len(os.listdir(f'{LOCAL_COCO}/train2014')):,} images\")\n",
    "print(f\"  val2014:   {len(os.listdir(f'{LOCAL_COCO}/val2014')):,} images\")\n",
    "print(f\"  Karpathy splits: {splits}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 4: Pull Latest Code + Build Frontend (static export)\n# git pull first so we build from the latest code (Cell 2 cloned a snapshot)\n!cd /content/vlm_quantization && git pull\n\n# Next.js 16 requires Node.js >= 18.18.0; Colab's default is too old\n!curl -fsSL https://deb.nodesource.com/setup_20.x | bash - > /dev/null 2>&1\n!apt-get -qq install -y nodejs > /dev/null 2>&1\n!node --version\n!cd /content/vlm_quantization/monitor/frontend && npm install --silent && npm run build\nprint(\"Frontend built → monitor/frontend/out/\")"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Dashboard: https://absorbed-efren-rubbly.ngrok-free.dev\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Start Monitoring Server + ngrok Tunnel\n",
    "import os\n",
    "import threading\n",
    "import time\n",
    "\n",
    "import uvicorn\n",
    "from pyngrok import ngrok\n",
    "\n",
    "token = os.environ.get(\"NGROK_AUTH_TOKEN\", \"\")\n",
    "if token:\n",
    "    ngrok.set_auth_token(token)\n",
    "\n",
    "def run_server():\n",
    "    uvicorn.run(\"monitor.server.app:app\", host=\"0.0.0.0\", port=8000, log_level=\"warning\")\n",
    "\n",
    "threading.Thread(target=run_server, daemon=True).start()\n",
    "time.sleep(3)\n",
    "\n",
    "tunnel = ngrok.connect(8000)\n",
    "print(f\"\\n Dashboard: {tunnel.public_url}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 6: Train (checkpoints → Google Drive)\n# git pull already done in Cell 4 before frontend build\n!cd /content/vlm_quantization && PYTHONPATH=/content/vlm_quantization python train.py --config configs/colab.yaml"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: (Optional) Optuna Hyperparameter Search\n",
    "# Runs 50 trials with 5 epochs each — takes several hours\n",
    "# Results DB + best config saved to Google Drive for persistence\n",
    "OPTUNA_DIR = \"/content/drive/MyDrive/vlm_quantization/optuna\"\n",
    "!mkdir -p {OPTUNA_DIR}\n",
    "!cd /content/vlm_quantization && git pull && PYTHONPATH=/content/vlm_quantization python optuna_search.py \\\n",
    "    --config configs/colab.yaml \\\n",
    "    --n-trials 50 \\\n",
    "    --storage sqlite:///{OPTUNA_DIR}/optuna_results.db \\\n",
    "    --export-config {OPTUNA_DIR}/best_config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Retrain with Best Optuna Config\n",
    "# Uses the best hyperparameters found by Optuna for full training\n",
    "OPTUNA_DIR = \"/content/drive/MyDrive/vlm_quantization/optuna\"\n",
    "BEST_CONFIG = f\"{OPTUNA_DIR}/best_config.yaml\"\n",
    "\n",
    "import os\n",
    "assert os.path.exists(BEST_CONFIG), f\"Best config not found: {BEST_CONFIG}\\nRun Cell 7 (Optuna search) first.\"\n",
    "\n",
    "print(\"Best config contents:\")\n",
    "!cat {BEST_CONFIG}\n",
    "print(\"\\n--- Starting full training with best hyperparameters ---\\n\")\n",
    "!cd /content/vlm_quantization && PYTHONPATH=/content/vlm_quantization python train.py --config {BEST_CONFIG}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}