model:
  backbone: "google/siglip2-so400m-patch14-384"
  bit_list: [8, 16, 32, 48, 64, 128]
  hidden_dim: 512
  shared_dim: 768                    # shared bottleneck dimension
  dropout: 0.1
  progressive_hash: true             # per-bit projection heads (vs prefix slicing)
  freeze_backbone: false
  use_lora: false                   # P2: LoRA fine-tuning (requires peft)
  lora_rank: 8
  lora_alpha: 16
  lora_dropout: 0.05

training:
  batch_size: 128
  max_epochs: 30
  hash_lr: 1.0e-3
  backbone_lr: 1.0e-5
  weight_decay: 0.01
  warmup_steps: 500
  gradient_clip_val: 1.0
  accumulate_grad_batches: 4
  early_stopping_patience: 5

loss:
  contrastive_weight: 1.0
  ortho_weight: 0.01                # reduced: margin handles alignment
  quantization_weight: 0.1
  balance_weight: 0.01
  consistency_weight: 0.5
  lcs_weight: 0.5
  distillation_weight: 1.0         # P0: backbone similarity distillation
  adapter_align_weight: 0.1        # adapter cosine alignment loss
  temperature: 0.07
  learnable_temp: true              # P4: learnable temperature (log-inverse param)
  focal_gamma: 2.0                  # P3: focal InfoNCE (0 = disabled)
  ortho_margin: 0.2                 # margin tolerance for off-diagonal similarity
  quantization_start_progress: 0.4  # two-stage: quant starts at 40% training
  distillation_teacher_temp: 0.1    # P0: teacher temperature
  distillation_student_temp: 0.05   # P0: student temperature
  ema_decay: 0.99

data:
  data_root: "./data/coco"
  karpathy_json: "./data/coco/dataset_coco.json"
  num_workers: 4
  max_text_length: 64
  image_size: 384
  num_captions: 2                   # P1: multi-caption contrastive (1 = disabled)
  text_dropout_prob: 0.1            # P5: text augmentation (0 = disabled)

monitor:
  enabled: true
  server_url: "http://localhost:8000"
  log_every_n_steps: 10
