model:
  backbone: "google/siglip2-so400m-patch14-384"
  bit_list: [8, 16, 32, 48, 64, 128]
  hidden_dim: 512
  shared_dim: 768                    # shared bottleneck dimension
  dropout: 0.1
  progressive_hash: true             # per-bit projection heads (vs prefix slicing)
  freeze_backbone: false          # A100: fine-tune backbone for Korean
  use_lora: false                 # P2: set true to enable LoRA (requires peft)
  lora_rank: 8
  lora_alpha: 16
  lora_dropout: 0.05

training:
  batch_size: auto                # auto-detect based on GPU VRAM
  max_epochs: 5                   # 6.4M samples Ã— 5 epochs
  hash_lr: 1.0e-3
  backbone_lr: 5.0e-6             # lower than default to prevent catastrophic forgetting
  weight_decay: 0.01
  warmup_steps: 500
  gradient_clip_val: 1.0
  accumulate_grad_batches: 4      # overridden when batch_size=auto
  val_check_interval: 0.5          # validate twice per epoch (float = fraction)
  early_stopping_patience: 3      # tighter than default (5)
  checkpoint_dir: "/content/drive/MyDrive/vlm_quantization/checkpoints"

loss:
  contrastive_weight: 1.0
  ortho_weight: 0.01                # reduced: margin handles alignment
  quantization_weight: 0.1
  balance_weight: 0.01
  consistency_weight: 0.5
  lcs_weight: 0.5
  distillation_weight: 1.0         # P0: backbone similarity distillation
  adapter_align_weight: 0.1        # adapter cosine alignment loss
  temperature: 0.07
  learnable_temp: true              # P4: learnable temperature (log-inverse param)
  focal_gamma: 2.0                  # P3: focal InfoNCE (0 = disabled)
  ortho_margin: 0.2                 # margin tolerance for off-diagonal similarity
  quantization_start_progress: 0.4  # two-stage: quant starts at 40% training
  distillation_teacher_temp: 0.1
  distillation_student_temp: 0.05
  ema_decay: 0.99

data:
  data_root: "/content/data/coco"
  karpathy_json: "/content/data/coco/dataset_coco.json"
  num_workers: 4                  # overridden when batch_size=auto
  max_text_length: 64
  image_size: 384
  num_captions: 2                   # P1: multi-caption contrastive
  text_dropout_prob: 0.1            # P5: text augmentation
  extra_datasets:
    - jsonl_path: "/content/data/aihub/aihub_71454.jsonl"
      data_root: "/content/data/aihub"
    - jsonl_path: "/content/data/cc3m_ko/cc3m_ko.jsonl"
      data_root: "/content/data/cc3m_ko"
    # Phase 2: LAION-KR (requires Drive 2TB or GCS)
    # - jsonl_path: "/content/data/laion_kr/laion_kr.jsonl"
    #   data_root: "/content/data/laion_kr"

monitor:
  enabled: true
  server_url: "http://localhost:8000"
  log_every_n_steps: 10
