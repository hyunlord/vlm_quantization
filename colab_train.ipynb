{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VLM Cross-Modal Deep Hashing — Training + Dashboard\n",
    "\n",
    "Unified notebook: Train a cross-modal hashing model (SigLIP2 → binary codes) with live dashboard.\n",
    "\n",
    "**Features:**\n",
    "- GPU training with real-time metrics\n",
    "- Live dashboard via ngrok (Training Progress, Loss Curves, System Stats)\n",
    "- Checkpoints saved to Google Drive\n",
    "- System stats show actual Colab GPU/RAM resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: GPU Check + Google Drive Mount\n",
    "import torch\n",
    "\n",
    "assert torch.cuda.is_available(), \"No GPU detected — switch to a GPU runtime.\"\n",
    "gpu_name = torch.cuda.get_device_name(0)\n",
    "vram = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "print(f\"GPU: {gpu_name} ({vram:.1f} GB)\")\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "!mkdir -p /content/drive/MyDrive/vlm_quantization/checkpoints\n",
    "!mkdir -p /content/drive/MyDrive/vlm_quantization/monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Clone Repo + Install Dependencies + Load .env\n",
    "!git clone https://github.com/hyunlord/vlm_quantization.git /content/vlm_quantization 2>/dev/null || true\n",
    "%cd /content/vlm_quantization\n",
    "!git pull\n",
    "!pip install -q -r requirements.txt\n",
    "!pip install -q pyngrok\n",
    "\n",
    "import os\n",
    "env_path = \"/content/drive/MyDrive/vlm_quantization/.env\"\n",
    "if os.path.exists(env_path):\n",
    "    with open(env_path) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line and not line.startswith(\"#\") and \"=\" in line:\n",
    "                key, val = line.split(\"=\", 1)\n",
    "                os.environ[key.strip()] = val.strip()\n",
    "    print(\".env loaded from Google Drive\")\n",
    "else:\n",
    "    print(f\"No .env found at {env_path} — create one if needed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Cell 3: Load COCO Dataset (zip cached on Drive, extracted to local SSD)\n",
    "import os, shutil\n",
    "\n",
    "DRIVE_CACHE = \"/content/drive/MyDrive/data/coco_zips\"\n",
    "LOCAL_COCO  = \"/content/data/coco\"\n",
    "\n",
    "SOURCES = {\n",
    "    \"train2014\": {\n",
    "        \"url\": \"http://images.cocodataset.org/zips/train2014.zip\",\n",
    "        \"zip\": \"train2014.zip\",\n",
    "        \"folder\": \"train2014\",\n",
    "    },\n",
    "    \"val2014\": {\n",
    "        \"url\": \"http://images.cocodataset.org/zips/val2014.zip\",\n",
    "        \"zip\": \"val2014.zip\",\n",
    "        \"folder\": \"val2014\",\n",
    "    },\n",
    "    \"annotations\": {\n",
    "        \"url\": \"http://images.cocodataset.org/annotations/annotations_trainval2014.zip\",\n",
    "        \"zip\": \"annotations_trainval2014.zip\",\n",
    "        \"folder\": \"annotations\",\n",
    "    },\n",
    "}\n",
    "\n",
    "os.makedirs(DRIVE_CACHE, exist_ok=True)\n",
    "os.makedirs(LOCAL_COCO, exist_ok=True)\n",
    "\n",
    "for i, (name, src) in enumerate(SOURCES.items(), 1):\n",
    "    local_dir = f\"{LOCAL_COCO}/{src['folder']}\"\n",
    "    drive_zip = f\"{DRIVE_CACHE}/{src['zip']}\"\n",
    "    tmp_zip   = f\"/tmp/{src['zip']}\"\n",
    "\n",
    "    if os.path.isdir(local_dir):\n",
    "        print(f\"  [{i}/3] {name} — already extracted locally, skipping\")\n",
    "        continue\n",
    "\n",
    "    if os.path.isfile(drive_zip):\n",
    "        print(f\"  [{i}/3] {name} — copying cached zip from Drive...\")\n",
    "        shutil.copy2(drive_zip, tmp_zip)\n",
    "    else:\n",
    "        print(f\"  [{i}/3] {name} — downloading...\")\n",
    "        !wget -q --show-progress {src['url']} -O {tmp_zip}\n",
    "        print(f\"         caching zip to Drive...\")\n",
    "        shutil.copy2(tmp_zip, drive_zip)\n",
    "\n",
    "    print(f\"         extracting to local disk...\")\n",
    "    !unzip -q {tmp_zip} -d {LOCAL_COCO}/\n",
    "    os.remove(tmp_zip)\n",
    "\n",
    "# Karpathy split JSON\n",
    "KARPATHY_JSON = f\"{LOCAL_COCO}/dataset_coco.json\"\n",
    "if not os.path.isfile(KARPATHY_JSON):\n",
    "    KARPATHY_URL = \"https://cs.stanford.edu/people/karpathy/deepimagesent/caption_datasets.zip\"\n",
    "    karpathy_zip = \"/tmp/caption_datasets.zip\"\n",
    "    drive_karpathy = f\"{DRIVE_CACHE}/caption_datasets.zip\"\n",
    "\n",
    "    if os.path.isfile(drive_karpathy):\n",
    "        print(\"  [K] Karpathy JSON — copying cached zip from Drive...\")\n",
    "        shutil.copy2(drive_karpathy, karpathy_zip)\n",
    "    else:\n",
    "        print(\"  [K] Karpathy JSON — downloading...\")\n",
    "        !wget -q --show-progress {KARPATHY_URL} -O {karpathy_zip}\n",
    "        print(\"         caching zip to Drive...\")\n",
    "        shutil.copy2(karpathy_zip, drive_karpathy)\n",
    "\n",
    "    print(\"         extracting dataset_coco.json...\")\n",
    "    !unzip -q -j {karpathy_zip} \"dataset_coco.json\" -d {LOCAL_COCO}/\n",
    "    os.remove(karpathy_zip)\n",
    "else:\n",
    "    print(\"  [K] Karpathy JSON — already present, skipping\")\n",
    "\n",
    "# Verify\n",
    "for name in (\"train2014\", \"val2014\", \"annotations\"):\n",
    "    assert os.path.isdir(f\"{LOCAL_COCO}/{name}\"), f\"{name} missing!\"\n",
    "assert os.path.isfile(KARPATHY_JSON), \"dataset_coco.json missing!\"\n",
    "\n",
    "import json\n",
    "with open(KARPATHY_JSON) as f:\n",
    "    kdata = json.load(f)\n",
    "splits = {}\n",
    "for img in kdata[\"images\"]:\n",
    "    s = img[\"split\"]\n",
    "    splits[s] = splits.get(s, 0) + 1\n",
    "del kdata\n",
    "\n",
    "print(f\"\\nCOCO ready: {LOCAL_COCO}\")\n",
    "print(f\"  train2014: {len(os.listdir(f'{LOCAL_COCO}/train2014')):,} images\")\n",
    "print(f\"  val2014:   {len(os.listdir(f'{LOCAL_COCO}/val2014')):,} images\")\n",
    "print(f\"  Karpathy splits: {splits}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Build Frontend (static export for dashboard)\n",
    "!curl -fsSL https://deb.nodesource.com/setup_20.x | bash - > /dev/null 2>&1\n",
    "!apt-get -qq install -y nodejs > /dev/null 2>&1\n",
    "print(f\"Node.js version: \", end=\"\")\n",
    "!node --version\n",
    "!cd /content/vlm_quantization/monitor/frontend && npm install --silent 2>/dev/null && npm run build 2>/dev/null\n",
    "print(\"Frontend built -> monitor/frontend/out/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Start Monitoring Server + ngrok Dashboard\n",
    "import os\n",
    "import threading\n",
    "import time\n",
    "\n",
    "import uvicorn\n",
    "from pyngrok import ngrok\n",
    "\n",
    "# Environment setup - metrics DB and checkpoints on Google Drive\n",
    "MONITOR_DIR = \"/content/drive/MyDrive/vlm_quantization/monitor\"\n",
    "CKPT_DIR = \"/content/drive/MyDrive/vlm_quantization/checkpoints\"\n",
    "os.makedirs(MONITOR_DIR, exist_ok=True)\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "\n",
    "os.environ[\"MONITOR_DB_PATH\"] = f\"{MONITOR_DIR}/metrics.db\"\n",
    "os.environ[\"CHECKPOINT_DIR\"] = CKPT_DIR\n",
    "\n",
    "# Start server in background thread\n",
    "def run_server():\n",
    "    uvicorn.run(\"monitor.server.app:app\", host=\"0.0.0.0\", port=8000, log_level=\"warning\")\n",
    "\n",
    "server_thread = threading.Thread(target=run_server, daemon=True)\n",
    "server_thread.start()\n",
    "time.sleep(3)\n",
    "\n",
    "# Setup ngrok tunnel\n",
    "token = os.environ.get(\"NGROK_AUTH_TOKEN\", \"\")\n",
    "if token:\n",
    "    ngrok.set_auth_token(token)\n",
    "\n",
    "try:\n",
    "    tunnel = ngrok.connect(8000)\n",
    "    dashboard_url = tunnel.public_url\n",
    "    print(f\"=\"*60)\n",
    "    print(f\"  DASHBOARD: {dashboard_url}\")\n",
    "    print(f\"=\"*60)\n",
    "    print(f\"  Metrics DB: {os.environ['MONITOR_DB_PATH']}\")\n",
    "    print(f\"  Checkpoints: {CKPT_DIR}\")\n",
    "    print(f\"\\n  Training metrics will appear in real-time!\")\n",
    "    print(f\"  System stats show THIS Colab's GPU/RAM.\")\n",
    "except Exception as e:\n",
    "    err_msg = str(e)\n",
    "    if \"ERR_NGROK_8012\" in err_msg:\n",
    "        print(\"ERROR: ngrok tunnel failed - server not running on port 8000\")\n",
    "    elif \"ERR_NGROK_334\" in err_msg or \"endpoint already\" in err_msg.lower():\n",
    "        print(\"ERROR: ngrok endpoint already in use.\")\n",
    "        print(\"  -> Restart runtime or kill existing tunnel\")\n",
    "    else:\n",
    "        print(f\"ngrok error: {e}\")\n",
    "    print(f\"\\nServer running locally on http://localhost:8000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Start Training\n",
    "# Metrics are sent to localhost:8000 -> visible in dashboard via ngrok\n",
    "# Checkpoints saved to Google Drive\n",
    "!cd /content/vlm_quantization && PYTHONPATH=/content/vlm_quantization python train.py --config configs/colab.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: (Optional) Optuna Hyperparameter Search\n",
    "# Runs 50 trials with 5 epochs each — takes several hours\n",
    "OPTUNA_DIR = \"/content/drive/MyDrive/vlm_quantization/optuna\"\n",
    "!mkdir -p {OPTUNA_DIR}\n",
    "!cd /content/vlm_quantization && git pull && PYTHONPATH=/content/vlm_quantization python optuna_search.py \\\n",
    "    --config configs/colab.yaml \\\n",
    "    --n-trials 50 \\\n",
    "    --storage sqlite:///{OPTUNA_DIR}/optuna_results.db \\\n",
    "    --export-config {OPTUNA_DIR}/best_config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Retrain with Best Optuna Config\n",
    "OPTUNA_DIR = \"/content/drive/MyDrive/vlm_quantization/optuna\"\n",
    "BEST_CONFIG = f\"{OPTUNA_DIR}/best_config.yaml\"\n",
    "\n",
    "import os\n",
    "assert os.path.exists(BEST_CONFIG), f\"Best config not found: {BEST_CONFIG}\\nRun Cell 7 (Optuna search) first.\"\n",
    "\n",
    "print(\"Best config contents:\")\n",
    "!cat {BEST_CONFIG}\n",
    "print(\"\\n--- Starting full training with best hyperparameters ---\\n\")\n",
    "!cd /content/vlm_quantization && PYTHONPATH=/content/vlm_quantization python train.py --config {BEST_CONFIG}"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
